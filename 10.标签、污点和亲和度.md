### 标签

#### 什么是标签

标签其实就一对 key/value ，被关联到对象上，比如Pod,标签的使用我们倾向于能够表示对象的特殊特点，就是一眼就看出了这个Pod是干什么的，标签可以用来划分特定的对象（比如版本，服务类型等），标签可以在创建一个对象的时候直接定义，也可以在后期随时修改，每一个对象可以拥有多个标签，但是，key值必须是唯一的。创建标签之后也可以方便我们对资源进行分组管理。如果对pod打标签，之后就可以使用标签来查看、删除指定的pod。

在k8s中，大部分资源都可以打标签。

#### 如何打标签

##### 用法

```sh
kubectl label pods pod-tomcat release=v1
```

```sh
kubectl get pods pod-tomcat --show-labels
```

##### 查看

```sh
#查看默认名称空间下所有pod资源的标签
kubectl get pods --show-labels 

#查看默认名称空间下指定pod具有的所有标签
kubectl get pods pod-first --show-labels

#列出默认名称空间下标签key是release的pod，不显示标签
kubectl get pods -l release

#列出默认名称空间下标签key是release、值是v1的pod，不显示标签
kubectl get pods -l release=v1

#查看所有名称空间下的所有pod的标签
kubectl get pods --all-namespaces --show-labels

```

### pod资源清单详细解读

```yml

apiVersion: v1       #版本号，例如v1
kind: Pod       #资源类型，如Pod
metadata:       #元数据
  name: string       # Pod名字
  namespace: string    # Pod所属的命名空间
  labels:      #自定义标签
    - name: string     #自定义标签名字
  annotations:       #自定义注释列表
    - name: string
spec:         # Pod中容器的详细定义
  containers:      # Pod中容器列表
  - name: string     #容器名称
    image: string    #容器的镜像名称
    imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像
    command: [string]    #容器的启动命令列表，如不指定，使用打包时使用的启动命令
    args: [string]     #容器的启动命令参数列表
    workingDir: string     #容器的工作目录
    volumeMounts:    #挂载到容器内部的存储卷配置
    - name: string     #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名
      mountPath: string    #存储卷在容器内mount的绝对路径，应少于512字符
      readOnly: boolean    #是否为只读模式
    ports:       #需要暴露的端口库号
    - name: string     #端口号名称
      containerPort: int   #容器需要监听的端口号
      hostPort: int    #容器所在主机需要监听的端口号，默认与Container相同
      protocol: string     #端口协议，支持TCP和UDP，默认TCP
    env:       #容器运行前需设置的环境变量列表
    - name: string     #环境变量名称
      value: string    #环境变量的值
    resources:       #资源限制和请求的设置
      limits:      #资源限制的设置
        cpu: string    #cpu的限制，单位为core数
        memory: string     #内存限制，单位可以为Mib/Gib
      requests:      #资源请求的设置
        cpu: string    #cpu请求，容器启动的初始可用数量
        memory: string     #内存请求，容器启动的初始可用内存
    livenessProbe:     #对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可
      exec:      #对Pod容器内检查方式设置为exec方式
        command: [string]  #exec方式需要制定的命令或脚本
      httpGet:       #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port
        path: string
        port: number
        host: string
        scheme: string
        HttpHeaders:
        - name: string
          value: string
      tcpSocket:     #对Pod内个容器健康检查方式设置为tcpSocket方式
         port: number
       initialDelaySeconds: 0  #容器启动完成后首次探测的时间，单位为秒
       timeoutSeconds: 0   #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒
       periodSeconds: 0    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次
       successThreshold: 0
       failureThreshold: 0
       securityContext:
         privileged:false
    restartPolicy: [Always | Never | OnFailure]#Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod
    nodeSelector: obeject  #设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定
    imagePullSecrets:    #Pull镜像时使用的secret名称，以key：secretkey格式指定
    - name: string
    hostNetwork:false      #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络
    volumes:       #在该pod上定义共享存储卷列表
    - name: string     #共享存储卷名称 （volumes类型有很多种）
      emptyDir: {}     #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值
      hostPath: string     #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录
        path: string     #Pod所在宿主机的目录，将被用于同期中mount的目录
      secret:      #类型为secret的存储卷，挂载集群与定义的secre对象到容器内部
        scretname: string  
        items:     
        - key: string
          path: string
      configMap:     #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部
        name: string
        items:
        - key: string
          path: string

```

### node节点选择器

我们在创建pod资源的时候，pod会根据schduler进行调度，那么默认会调度到随机的一个工作节点，如果我们想要pod调度到指定节点或者调度到一些具有相同特点的node节点，怎么办呢？

可以使用pod中的nodeName或者nodeSelector字段指定要调度到的node节点

```sh
vim demo-pod.yaml 
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: demo-pod
  namespace: default
  labels:
    app: myapp
    env: dev
spec:
  nodeName: tomcat-node
  containers:
  - name:  tomcat-pod-java
    ports:
    - containerPort: 8080
    image: tomcat:8.5-jre8-alpine
    imagePullPolicy: IfNotPresent
  - name: busybox
    image: busybox:latest
    command:
    - "/bin/sh"
    - "-c"
	- "sleep 3600"
```

```sh
kubectl	apply -f demo-pod.yaml
```

将node2工作节点打上disk=ceph标签

```
kubectl label nodes node2 disk=ceph
```

创建新pod将nodeSelector设置为disk:ceph

```sh
vim pod-1.yaml
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: demo-pod-1
  namespace: default
  labels:
    app: myapp
    env: dev
spec:
  nodeSelector:
    disk: ceph
  containers:
  - name:  tomcat-pod-java
    ports:
    - containerPort: 8080
    image: tomcat:8.5-jre8-alpine
    imagePullPolicy: IfNotPresent
```

```sh
kubectl apply -f pod-1.yaml
```

可查看截图看到选择了node2节点。

![image-20220711144859122](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711144859122.png)

### 亲和性

在k8s中，pod会通过 kube-scheduler 按照节点现有的资源平均调度到这些节点上，但有时候，我们需要将某个应用的pod调度到特定的节点上。比如：两个应用需要频繁的进行通讯，那么我们希望将它们部署到同一个节点。或者希望访问一些类似需要ssd这样特殊资源的节点等应用场景

- 亲和性：应用A与应用B两个应用频繁交互，所以有必要利用亲和性让两个应用的尽可能的靠近，甚至在一个node上，以减少因网络通信而带来的性能损耗。
- 反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，以提高HA

#### node节点亲和性

node节点亲和性调度:nodeAffinity

```sh
kubectl explain pods.spec.affinity
```

![image-20220711145128101](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711145128101.png)

从中可以看到内容包括nodeAffinity、podAffinity和podAntiAffinity。

继续往下面看

```sh
kubectl explain  pods.spec.affinity.nodeAffinity
```

![image-20220711145253664](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711145253664.png)

这里分为两类：偏好和必要。在往必要往下看。

```sh
kubectl explain pods.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution
```

![image-20220711150508908](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711150508908.png)

```sh
kubectl explain pods.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms
```

![image-20220711153916743](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711153916743.png)

匹配表达式和匹配字段

```sh
kubectl explain pods.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchFields
```

![image-20220711154035909](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711154035909.png)

```sh
key：检查label
operator：做等值选则还是不等值选则
values：给定值
```

##### requiredDuringSchedulingIgnoredDuringExecution实验步骤

创建pod-node-affinity-demo的pod

```yaml
apiVersion: v1
kind: Pod
metadata:
        name: pod-node-affinity-demo
        namespace: default
        labels:
            app: myapp
            tier: frontend
spec:
    containers:
    - name: myapp
      image: ikubernetes/myapp:v1
    affinity:
        nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
                   nodeSelectorTerms:
                   - matchExpressions:
                     - key: zone
                       operator: In
                       values:
                       - foo
                       - bar
```

如果直接启动会出现pending状态。

通过`describe`

```sh
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  49s   default-scheduler  0/4 nodes are available: 2 node(s) didn't match Pod's node affinity, 2 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.
  Warning  FailedScheduling  49s   default-scheduler  0/4 nodes are available: 2 node(s) didn't match Pod's node affinity, 2 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.

```

这是因为现有的两个node都没有标签。

```sh
kubectl label nodes node2 zone=foo
```

一设置，就可以看到启动成功。

![image-20220711154839698](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711154839698.png)

##### preferredDuringSchedulingIgnoredDuringExecution实验步骤

创建pod-node-affinity-demo-2

```yaml
apiVersion: v1
kind: Pod
metadata:
        name: pod-node-affinity-demo-2
        namespace: default
        labels:
            app: myapp
            tier: frontend
spec:
    containers:
    - name: myapp
      image: ikubernetes/myapp:v1
    affinity:
        nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
               matchExpressions:
               - key: zone1
                 operator: In
                 values:
                 - foo1
                 - bar1
              weight: 60
```

如果直接启动，发现启动成功。这是因为所有节点都不满足条件，直接根据调度算法随机的。

```sh
kubectl label node node zone1=foo1
```

修改了以后，也不会转移node。

但是如果删除了以后，重新启动就会发现跑到node运行了。

![image-20220711160128016](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711160128016.png)

上面说明软亲和性是可以运行这个pod的，尽管没有运行这个pod的节点定义的zone1标签。

Node节点亲和性针对的是pod和node的关系，Pod调度到node节点的时候匹配的条件。

#### pod节点亲和性

pod自身的亲和性调度有两种表示形式

- podaffinity：pod和pod更倾向腻在一起，把相近的pod结合到相近的位置，如同一区域，同一机架，这样的话pod和pod之间更好通信，比方说有两个机房，这两个机房部署的集群有1000台主机，那么我们希望把nginx和tomcat都部署同一个地方的node节点上，可以提高通信效率；
- podunaffinity：pod和pod更倾向不腻在一起，如果部署两套程序，那么这两套程序更倾向于反亲和性，这样相互之间不会有影响。

第一个pod随机选则一个节点，做为评判后续的pod能否到达这个pod所在的节点上的运行方式，这就称为pod亲和性；我们怎么判定哪些节点是相同位置的，哪些节点是不同位置的；我们在定义pod亲和性时需要有一个前提，哪些pod在同一个位置，哪些pod不在同一个位置，这个位置是怎么定义的，标准是什么？以节点名称为标准，这个节点名称相同的表示是同一个位置，节点名称不相同的表示不是一个位置。

```sh
kubectl explain pods.spec.affinity.podAffinity
```

和node亲和性一样，分为硬亲和性和软亲和性。

![image-20220711165344954](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711165344954.png)

```sh
kubectl explain pods.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution.labelSelector
```

![image-20220711161308263](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711161308263.png)

开始实验

创建pod-first和pod-second

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-first
  labels:
    app2: myapp2
    tier: frontend
spec:
    containers:
    - name: myapp
      image: ikubernetes/myapp:v1
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-second
  labels:
    app: backend
    tier: db
spec:
    containers:
    - name: busybox
      image: busybox:latest
      imagePullPolicy: IfNotPresent
      command: ["sh","-c","sleep 3600"]
    affinity:
      podAffinity:
         requiredDuringSchedulingIgnoredDuringExecution:
         - labelSelector:
              matchExpressions:
              - {key: app2, operator: In, values: ["myapp2"]}
           topologyKey: kubernetes.io/hostname
```

`---`以上是第一个pod，随机调度到node。

以下是第二个pod。匹配标签:app2 in ("myapp2")的pod，只要有`kubernetes.io/hostname`的值一致算同一个位置。其实这个`kubernetes.io/hostname`是都有的。

![image-20220711164510285](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711164510285.png)

##### 反亲和性

如果将这个设置成反亲和性就会发现pod-second启动不了。这是因为node和node2都有`kubernetes.io/hostname`在逻辑上属于一个位置。然后pod-first安装到任意节点，都算是一个位置。这样就会导致second启动失败。

```sh
vim pod-required-anti-affinity-demo.yaml
```

![image-20220711163132242](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711163132242.png)

在这里以app2作为反亲和节点

cat pod-required-anti-affinity-demo.yaml

```sh

apiVersion: v1
kind: Pod
metadata:
  name: pod-first
  labels:
    app1: myapp1
    tier: frontend
spec:
    containers:
    - name: myapp
      image: ikubernetes/myapp:v1
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-anti-second
  labels:
    app: backend
    tier: db
spec:
    containers:
    - name: busybox
      image: busybox:latest
      imagePullPolicy: IfNotPresent
      command: ["sh","-c","sleep 3600"]
    affinity:
      podAntiAffinity:
         requiredDuringSchedulingIgnoredDuringExecution:
         - labelSelector:
              matchExpressions:
              - {key: app1, operator: In, values: ["myapp1"]}
           topologyKey: kubernetes.io/hostname
```

如果更换topologyKey为自定义的键，如将两个工作节点都标记env=test

```sh
[root@master1 ~]# kubectl label node node env=test
node/node labeled
[root@master1 ~]# kubectl label node node2 env=test
node/node2 labeled
```

然后删除，重启，会发现pod-anti-second无法启动。

![image-20220711165027053](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220711165027053.png)

这是因为两个节点都是env=test一样的，属于逻辑一样的。

### 污点、容忍度

给了节点选择的主动权，我们给节点打一个污点，不容忍的pod就运行不上来，污点就是定义在节点上的键值属性数据，可以定决定拒绝那些pod；

- taints是键值数据，用在节点上，定义污点；
- tolerations是键值数据，用在pod上，定义容忍度，能容忍哪些污点

pod亲和性是pod属性；但是污点是节点的属性，污点定义在nodeSelector上

```sh
kubectl explain node.spec.taints
```

![image-20220712100023862](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220712100023862.png)

taints的effect用来定义对pod对象的排斥等级（效果）：

```sh
Required. The effect of the taint on pods that do not tolerate the taint.
Valid effects are NoSchedule, PreferNoSchedule and NoExecute.
```

NoSchedule：

仅影响pod调度过程，当pod能容忍这个节点污点，就可以调度到当前节点，后来这个节点的污点改了，加了一个新的污点，使得之前调度的pod不能容忍了，那这个pod会怎么处理，对现存的pod对象不产生影响

NoExecute：

既影响调度过程，又影响现存的pod对象，如果现存的pod不能容忍节点后来加的污点，这个pod就会被驱逐

PreferNoSchedule：

最好不，也可以，是NoSchedule的柔性版本

在pod对象定义容忍度的时候支持两种操作：

- 等值密钥：key和value上完全匹配
- 存在性判断：key和effect必须同时匹配，value可以是空

在pod上定义的容忍度可能不止一个，在节点上定义的污点可能多个，需要琢个检查容忍度和污点能否匹配，每一个污点都能被容忍，才能完成调度，如果不能容忍怎么办，那就需要看pod的容忍度了。

默认情况下，default命名空间，node节点是没有污点的。

```sh
[root@master1 ~]# kubectl describe nodes node|grep Taints
Taints:             <none>
```

再看看kube-system的pods，看看容忍度如何

![image-20220712101033052](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220712101033052.png)

有图可以看出能够容忍NoExecute,然后再看看kube-system命名空间所在的控制节点的污点。

![image-20220712101413230](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220712101413230.png)

#### 实验步骤

将节点2设置为生产环境

```sh
 kubectl taint node node2 nn-type=production:NoSchedule
```

然后再看看影响到的pods

![image-20220712101717595](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220712101717595.png)

其中可以看出pods都跑到node去了，唯一一个反亲和的pods无法启动（因为目标反亲和pods）。

创建pod并启动

```sh
[root@master1 ~]# cat pod-taint.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: taint-pod
  namespace: default
  labels:
    tomcat:  tomcat-pod
spec:
  containers:
  - name:  taint-pod
    ports:
    - containerPort: 8080
    image: tomcat:8.5-jre8-alpine
    imagePullPolicy: IfNotPresent    
```

启动以后，发现位置再node节点。这是因为node2节点是有污点的。如果再把node节点设置为dev环境。这会导致现有的pods都被赶走了。

```
kubectl taint node node nn-type=dev:NoExecute
```

```sh
kubectl get pods -o wide 
```

![image-20220712105253537](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220712105253537.png)

这里面的永远也启动不了，因为不能容忍两个node的污点。

重新创建一个

```sh
apiVersion: v1
kind: Pod
metadata:
  name: myapp-deploy
  namespace: default
  labels:
    app: myapp
    release: canary
spec:
      containers:
      - name: myapp
        image: ikubernetes/myapp:v1
        ports:
        - name: http
          containerPort: 80
      tolerations:
      - key: "nn-type"
        operator: "Equal"
        value: "production"
        effect: "NoExecute"
        tolerationSeconds: 3600
```

启动以后，发现并没有想象到的结果，讲道理应该再node2，因为NoExecute更加严格才对。

![image-20220712105847472](10.%E6%A0%87%E7%AD%BE%E3%80%81%E6%B1%A1%E7%82%B9%E5%92%8C%E4%BA%B2%E5%92%8C%E5%BA%A6.assets/image-20220712105847472.png)

这是因为使用了Equal。这样的话就必须完全一样才行。更正effect: "NoExecute"为effect: "NoSchedule"，并去掉tolerationSeconds。

```sh
tolerationSeconds: 3600
```

删掉后，并重启发现处于RUNNING状态。

或者将Equal修改为exists,然后去掉value。这是因为value会被自动定义成通配符。

```sh
tolerations:
- key: "nn-type"
operator: "Exists"
value: ""
effect: "NoSchedule"
```

重新调度以后，还是再node2节点。

再次修改

```sh
tolerations:
- key: "nn-type"
operator: "Exists"
value: ""
effect: ""
```

这样表示，只要有`nn-type`污点都能容忍。

测试完毕以后，把污点都删掉。

```sh
kubectl taint nodes node1 nn-type:Execute-
kubectl taint nodes node2 nn-type-
```



### Pod状态

Pod的status定义在[PodStatus](https://link.zhihu.com/?target=https%3A//kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/%23podstatus-v1-core)对象中，其中有一个phase字段。它简单描述了Pod在其生命周期的阶段。熟悉Pod的各种状态对我们理解如何设置Pod的调度策略、重启策略是很有必要的。下面是 phase 可能的值，也就是pod常见的状态：

- 挂起（Pending）：我们在请求创建pod时，条件不满足，调度没有完成，没有任何一个节点能满足调度条件，已经创建了pod但是没有适合它运行的节点叫做挂起，调度没有完成，处于pending的状态会持续一段时间：包括调度Pod的时间和通过网络下载镜像的时间。 
- 运行中（Running）：Pod已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。
- 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。
- 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。
- 未知（Unknown）：未知状态，所谓pod是什么状态是apiserver和运行在pod节点的kubelet进行通信获取状态信息的，如果节点之上的kubelet本身出故障，那么apiserver就连不上kubelet，得不到信息了，就会看Unknown

扩展：还有其他状态，如下：

- Evicted状态：出现这种情况，多见于系统内存或硬盘资源不足，可df-h查看docker存储所在目录的资源使用情况，如果百分比大于85%，就要及时清理下资源，尤其是一些大文件、docker镜像。
- CrashLoopBackOff：容器曾经启动了，但可能又异常退出了
- Error 状态：Pod 启动过程中发生了错误



### Pod重启策略

Pod的重启策略（RestartPolicy）应用于Pod内的所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据 RestartPolicy 的设置来进行相应的操作。

Pod的重启策略包括 Always、OnFailure和Never，默认值为Always。

- Always：当容器失败时，由kubelet自动重启该容器。

- OnFailure：当容器终止运行且退出码不为0时，由kubelet自动重启该容器。

- Never：不论容器运行状态如何，kubelet都不会重启该容器。

```yaml
[root@xianchaomaster1 ~]# vim pod.yaml 
apiVersion: v1
kind: Pod
metadata:
 name: demo-pod
 namespace: default
 labels:
  app: myapp
spec:
 restartPolicy: Always
 containers:
 - name: tomcat-pod-java
  ports:
  - containerPort: 8080
  image: tomcat:8.5-jre8-alpine
  imagePullPolicy: IfNotPresent 
```

